import sys
import configparser
import web_clinic
import json

# Azure Text Analytics
from azure.core.credentials import AzureKeyCredential
from azure.ai.textanalytics import (
    TextAnalyticsClient,
    ExtractKeyPhrasesAction,
    AnalyzeSentimentAction
)

from flask import Flask, request, abort, render_template
from linebot.v3 import (
    WebhookHandler
)
from linebot.v3.exceptions import (
    InvalidSignatureError
)
from linebot.v3.webhooks import (
    MessageEvent,
    TextMessageContent,
)
from linebot.v3.messaging import (
    Configuration,
    ApiClient,
    MessagingApi,
    ReplyMessageRequest,
    TextMessage
)

#Config Parser
config = configparser.ConfigParser()
config.read('config.ini')

# Azure Text Analytics
credential = AzureKeyCredential(config["AzureLanguage"]["API_KEY"])


app = Flask(__name__)

channel_access_token = config['Line']['CHANNEL_ACCESS_TOKEN']
channel_secret = config['Line']['CHANNEL_SECRET']
if channel_secret is None:
    print('Specify LINE_CHANNEL_SECRET as environment variable.')
    sys.exit(1)
if channel_access_token is None:
    print('Specify LINE_CHANNEL_ACCESS_TOKEN as environment variable.')
    sys.exit(1)

handler = WebhookHandler(channel_secret)

configuration = Configuration(
    access_token=channel_access_token
)

@app.route("/callback", methods=['POST'])
def callback():
    # get X-Line-Signature header value
    signature = request.headers['X-Line-Signature']
    # get request body as text
    body = request.get_data(as_text=True)
    app.logger.info("Request body: " + body)

    # parse webhook body
    try:
        handler.handle(body, signature)
    except InvalidSignatureError:
        abort(400)
    return 'OK'

@app.route("/")
def home():
    return render_template('home.html')

@app.route("/submit", methods=['POST'])
def submit():
    review = {
        "positive": 0,
        "negative": 0,
        "neutral" :0
    }
    web_clinic.reset()
    finalresult =  []
    if request.method == 'POST':
        id = request.form['url'].split('!1s')[1].split('!8m2!')[0]

    # result = web_clinic.get_20_reviews(id)
    name = web_clinic.fetch_name()


    # a = azure_sentiment(result[1], 'web')
    # a['time'] = result[1]['time']        
    # finalresult.append(a)

    #for res in result:
    #    a = azure_sentiment(res, 'web')
    #    a['time'] = res['time']        
    #    finalresult.append(a)
    
    finalresult = [{'review': 'ä¸æ‡‚ç‚ºä»€éº¼èªªæ—é†«ç”Ÿä¸å¥½ï¼Œä»Šæ—©çœ‹éè¦ºå¾—æ…‹åº¦å¾ˆå‹å–„ä¹Ÿæœƒå¯’æš„ï¼Œçœ‹è¨ºé€Ÿåº¦å¿«ï¼Œæ•´é«”è¨ºæ‰€ç’°å¢ƒèˆ’é©ï¼Œçœ‹è¨ºå¾Œè—¥å¸«ä¹Ÿæœƒè§£èªªè—¥ç‰©ï¼Œç¶“é©—é‚„ä¸éŒ¯(ğŸ˜Š , + 1)\n', 'total': 1, 'key_phrases': ['æ—é†«ç”Ÿ', 'æ…‹åº¦', 'çœ‹è¨º', 'é€Ÿåº¦', 'æ•´é«”è¨ºæ‰€ç’°å¢ƒ', 'è—¥å¸«', 'è—¥ç‰©', 'ç¶“é©—'], 'time': '1 å€‹æœˆå‰'}, {'review': 'é†«ç”Ÿå¾ˆå¥½ï¼Œä½†æ˜¯æ™šç­å…©ä½æ«ƒå°æ…‹åº¦æ¥µå·®ï¼Œå·²ç¶“æœ‰é ç´„ï¼Œè¦æˆ‘å€‘æ™šé»åˆ°ï¼Œå®Œäº”åˆ†é˜åˆ°å°±ä¸çµ¦æ›è™Ÿï¼Œå°å­©ç™¼é«˜ç‡’äººä¸èˆ’æœï¼Œé‚„å …æŒä¸çµ¦æ›è™Ÿï¼Œåƒæ‹œè¨—è¬æ‹œè¨—å¥½ä¸å®¹æ˜“æ‰çµ¦æ›è™Ÿï¼Œè™Ÿç¢¼é‚„æ•…æ„å¾€å¾Œï¼Œå¥½åƒæ˜¯å¥¹å€‘æ–½æ©å…¸ä¼¼çš„ï¼Œæ…‹åº¦æ¥µå·®å°å¾…æˆ‘å€‘ï¼Œå°å­©å¾å°åˆ°å¤§éƒ½åœ¨é€™å®¶çœ‹ï¼Œç¬¬ä¸€æ¬¡é‡åˆ°é€™æ¨£è­·å£«ï¼Œå¤§å¤§å½±éŸ¿å°é€™å®¶è¨ºæ‰€å°è±¡(ğŸ˜  , - 1)\n', 'total': -1, 'key_phrases': ['é†«ç”Ÿ', 'æ™šç­å…©ä½æ«ƒå°æ…‹åº¦', 'é ç´„', 'å°å­©', 'é«˜ç‡’äºº', 'è™Ÿç¢¼', 'æ©å…¸', 'ç¬¬ä¸€æ¬¡', 'è­·å£«', 'è¨ºæ‰€'], 'time': '1 å€‹æœˆå‰'}, {'review': 'çµ¦éŒ¯çœ‹è¨ºè™Ÿç¢¼ï¼Œè¢«å¼·è¿«å¾€å¾Œå»¶ã€‚(ğŸ˜  , - 1)\nå¾€å¾Œå»¶ä¸€æ¬¡å¾Œï¼Œåˆèªªä¸æ˜¯æˆ‘ï¼Œå…©åº¦çµ¦éŒ¯çœ‹è¨ºè™Ÿç¢¼ï¼Ÿ(ğŸ˜  , - 1)\nèªªå‰›å‰›æœ‰å«è™Ÿä½†æˆ‘æ²’è½åˆ°ï¼Ÿ(ğŸ˜  , - 1)\næˆ‘æ˜æ˜å°±ååœ¨å®¤å…§ï¼Œä»€éº¼éƒ½æ²’è½åˆ°ï¼Œç›¯è‘—è™Ÿç¢¼ç‡ˆç›´æ¥è·³éæˆ‘çš„è™Ÿç¢¼ï¼Œç„¶å¾Œè¢«è¿«å†å¾€å¾Œå»¶ä¸€æ¬¡ï¼Ÿ(ğŸ˜  , - 1)\nçœŸçš„è«åå…¶å¦™ï¼Œæƒ³çœ‹è¨ºçš„äººè¦ä¸‰æ€ï¼Œä½ çš„è™Ÿç¢¼æœƒè¢«è«åçš„è¢«å¾€å¾Œå»¶ã€‚(ğŸ˜  , - 1)\n', 'total': -5, 'key_phrases': ['è¨ºè™Ÿç¢¼', 'çœ‹', 'å«è™Ÿ', 'å®¤å…§', 'è™Ÿç¢¼ç‡ˆ', 'äºº'], 'time': '1 å€‹æœˆå‰'}, {'review': 'æ˜¨å¤©åˆæ¬¡åˆ°è¨ºæ‰€çœ‹è¨ºï¼Œæ«ƒæª¯äººå“¡è·Ÿé†«ç”Ÿéƒ½éå¸¸è¦ªåˆ‡ä¹Ÿå¾ˆæœ‰è€å¿ƒã€‚(ğŸ˜Š , + 1)\næˆ‘è·Ÿé†«ç”Ÿèªªæˆ‘çœ‹éå¾ˆå¤šå®¶åƒäº†ä¸å°‘è—¥éƒ½æ²’æœ‰æ”¹å–„ã€‚(ğŸ˜  , - 1)\néš¨æ™‚éƒ½ä¹¾å’³éƒ½è¦ºå¾—å–‰åš¨æœ‰è¡€æœ‰ç•°ç‰©æ„Ÿã€ç—°éƒ½å’³ä¸å‡ºä¾†ã€ç˜‹ç‹‚é¼»å¡ä¹Ÿé¼»æ°´å€’æµã€ç¡å‰è·Ÿèµ·åºŠå¾Œéƒ½é¼»æ°´ç‹‚æµã€ç‹‚æ‰“å™´åšğŸ¤§ å°æ–¼ä¸€å€‹æ›å­£å°±æœƒéæ•çš„äººçœŸçš„å¾ˆé›£å—ï¼(ğŸ˜  , - 1)\nå›å®¶æœè—¥å¾ŒçœŸçš„å…¨éƒ¨çš„ç—‡ç‹€éƒ½æœ‰æ”¹å–„ï¼Œæœ€ä»¤äººå›°æ“¾çš„ç¡è¦ºé¼»å¡è·Ÿä¹¾å’³ç«Ÿç„¶éƒ½æ¶ˆå¤±äº†ï¼ï¼ï¼(ğŸ˜  , - 1)\nçµ‚æ–¼æœ‰ä¸€å€‹éå¸¸å¥½çš„ç¡è¦ºå“è³ªï¼(ğŸ˜Š , + 1)\nçœŸçš„éå¸¸æ„Ÿè¬å»ºä½‘è¨ºæ‰€çš„ç´°å¿ƒè€å¿ƒçœ‹è¨ºï¼Œä»¥åŠï¼ˆçœŸçš„ï¼‰å°ç—‡ä¸‹è—¥ï¼(ğŸ˜Š , + 1)\nå¾ˆå€¼å¾—çµ¦äºˆäº”æ˜Ÿå¥½è©•ï¼ï¼(ğŸ˜Š , + 1)\nâ™¡â™¡â™¡è”¡é†«å¸«è¬æ­²ï¼ï¼(ğŸ˜Š , + 1)\n', 'total': 2, 'key_phrases': ['æ˜¨å¤©', 'è¨ºæ‰€', 'æ«ƒæª¯äººå“¡', 'é†«ç”Ÿ', 'è€å¿ƒ', 'å¾ˆå¤šå®¶', 'äº†ä¸å°‘è—¥', 'æ”¹å–„', 'ä¹¾å’³', 'å–‰åš¨', 'è¡€', 'ç•°ç‰©æ„Ÿ', 'ç˜‹ç‹‚é¼»å¡', 'é¼»æ°´', 'èµ·åºŠ', 'å™´åš', 'ä¸€å€‹', 'å­£', 'ç—‡ç‹€', 'ç¡è¦ºé¼»å¡', 'ç¡è¦ºå“è³ª', 'äº”æ˜Ÿå¥½è©•', 'è”¡é†«å¸«'], 'time': '2 å€‹æœˆå‰'}, {'review': 'çœ‹è¨ºå°ˆæ¥­ã€æ…‹åº¦è¦ªåˆ‡é™³é†«å¸«è·Ÿè”¡é†«å¸«éƒ½éå¸¸ç”¨å¿ƒæ¨ï¼(ğŸ˜Š , + 1)\n', 'total': 1, 'key_phrases': ['æ…‹åº¦', 'é™³é†«å¸«', 'è”¡é†«å¸«'], 'time': '2 å€‹æœˆå‰'}, {'review': 'æº«æŸ”çš„è”¡é†«ç”Ÿæ‹¯æ•‘äº†æˆ‘å°‡è¿‘5å¤©çš„çœ©æšˆï¼ŒåŸæœ¬ä»¥ç‚ºæ²’æ•‘äº†ï¼ŒçœŸçš„ç—›è‹¦åˆ°å¾ˆæƒ³æ­»ï¼Œä½†æ˜¯ä»Šå¤©åƒå®Œè”¡é†«å¸«çš„è—¥æˆ‘å°±å¥½ä¸€åŠäº†ï¼Œäº¤çµ¦å°ˆæ¥­çš„å°±å°äº†  (ğŸ˜Š , + 1)\n', 'total': 1, 'key_phrases': ['è”¡é†«ç”Ÿ', 'çœ©æšˆ', 'ä»Šå¤©', 'è”¡é†«å¸«'], 'time': '2 å€‹æœˆå‰'}, {'review': 'æ²’å¸¶å£ç½©ï¼Œå£ç½©ä¸€å€‹è³£10å…ƒï¼Œå£ç½©æ²’åŒ…è£ï¼Œåˆä¸é–‹æ”¶æ“šï¼Œè­·å£«æœå‹™æ…‹åº¦å·®ï¼(ğŸ˜  , - 1)\n', 'total': -1, 'key_phrases': ['å£ç½©', 'åŒ…è£', 'æ”¶æ“š', 'è­·å£«æœå‹™æ…‹åº¦å·®'], 'time': '2 å€‹æœˆå‰'}, {'review': 'çœ‹è¨ºç…§é †åºèµ°å˜›éè™Ÿçš„äººå¡åœ¨æˆ‘è€å©†è™Ÿç¢¼å‰é¢å¡å¾ˆä¹…æˆ‘åœ¨å¤–é¢ç­‰éè™Ÿå¦³çš„äº‹æƒ…é—œæˆ‘ä»€éº¼äº‹æƒ…ï¼Ÿ(ğŸ˜  , - 1)\né†«ç”Ÿå»å¥½åƒåœ¨çµ¦æˆ‘è·Ÿç—…äººèŠå¤©ä¸€å€‹çœ‹è¨ºè‡³å°‘15åˆ†èµ·è·³æ˜¯æŠŠè„ˆé‚„æ˜¯çœ‹ç”Ÿè¾°å…«å­—ï¼Ÿ(ğŸ˜¶ , + 0)\næ›æˆ‘è€å©†æ™‚  èªªäº†ä¸€å¥ç™¼ç‡’  æ¸¬å¿«ç¯©è®“å¥¹åœ¨ç­‰å…©å€‹è™Ÿå¾ˆçæ¬¸    éè™Ÿçš„äººå¯ä»¥å¡åœ¨è£¡é¢é‚£éº¼ä¹…æˆ‘å»ä¸€ç­‰å†ç­‰  ä¸æ˜¯ä¸é¡˜æ„ç­‰éº»ç…©æœ‰é»é‚è¼¯çš„çœ‹è¨ºå¥½å—ï¼Ÿ(ğŸ˜  , - 1)\nå¾90å¹¾è™Ÿç­‰åˆ°1ç™¾ä¸€åå¤šè™Ÿç…§æ­£å¸¸ç¨‹åºèµ°  æˆ‘ä¸æœƒæ€æ¨£ä½†é€™é»è®“æˆ‘è¦ºå¾—åˆ°åº•æ˜¯çœ‹è¨ºçš„è¨ºæ‰€é‚„æ˜¯ä¾†èŠå¤©çš„å–èŒ¶åœ°æ–¹ï¼Ÿ(ğŸ˜¶ , + 0)\n', 'total': -2, 'key_phrases': ['äºº', 'è€å©†è™Ÿç¢¼', 'äº‹æƒ…', 'é†«ç”Ÿ', '15åˆ†', 'ç”Ÿè¾°å…«å­—', 'ä¸€å¥', 'ç‡’', 'å…©å€‹è™Ÿ', 'è£¡é¢', 'éº»ç…©', 'é‚è¼¯', '1ç™¾ä¸€åå¤šè™Ÿç…§æ­£å¸¸ç¨‹åº', 'è¨ºæ‰€', 'èŒ¶åœ°æ–¹'], 'time': '2 å€‹æœˆå‰'}, {'review': 'è”¡é†«å¸«è¶…å¼·çš„ï¼ï¼ï¼(ğŸ˜¶ , + 0)\næˆ‘å¿ƒä¸­è€³é¼»å–‰ç§‘çš„ç¬¬ä¸€åï¼Œè€Œä¸”åœ¨çœ©æšˆç—‡çš„é ˜åŸŸä¹Ÿæ˜¯å°ˆç²¾ã€‚(ğŸ˜¶ , + 0)\nè¶…æ„›å•è”¡é†«å¸«å•é¡Œçš„ï¼Œæ ¹æœ¬æ˜¯å°ç™¾ç§‘ï¼ï¼(ğŸ˜Š , + 1)\nğŸ˜ğŸ˜ğŸ˜(ğŸ˜Š , + 1)\n', 'total': 2, 'key_phrases': ['è”¡é†«å¸«', 'ç¬¬ä¸€å', 'é ˜åŸŸ', 'å°ç™¾ç§‘'], 'time': '2 å€‹æœˆå‰'}, {'review': 'è”¡é†«å¸«è¶…å°ˆæ¥­(ğŸ˜Š , + 1)\n', 'total': 1, 'key_phrases': ['è”¡é†«å¸«è¶…å°ˆæ¥­'], 'time': '3 å€‹æœˆå‰'}, {'review': 'ä¸€å®¶äººçœ‹ç—…çš„æŒ‡å®šè¨ºæ‰€ï¼Œé™³é†«ç”Ÿè¶…èªçœŸåˆç†±å¿ƒï¼Œè”¡é†«ç”Ÿä¹Ÿæ˜¯ï¼Œé€™å®¶è¨ºæ‰€å¾ˆæ¨è–¦ï¼(ğŸ˜Š , + 1)\n', 'total': 1, 'key_phrases': ['ä¸€å®¶äºº', 'æŒ‡å®šè¨ºæ‰€', 'é™³é†«ç”Ÿ', 'è”¡é†«ç”Ÿ'], 'time': '3 å€‹æœˆå‰'}, {'review': 'é†«ç”Ÿå€‘å•è¨ºç´°å¿ƒå°ˆæ¥­ï¼Œè¦ªåˆ‡åˆæœ‰è€å¿ƒï¼Œæœ€é‡è¦æ˜¯èƒ½å°ç—‡ä¸‹è—¥ï¼Œè®“èº«é«”ä¸èˆ’æœå®¹æ˜“ç»ç’ƒå¿ƒçš„ç—…äººå¿ƒæƒ…æ„‰æ‚…ï¼Œç—…å…ˆå¥½ä¸€åŠâ¤ï¸(ğŸ˜Š , + 1)\n', 'total': 1, 'key_phrases': ['é†«ç”Ÿå€‘', 'è€å¿ƒ', 'è—¥', 'èº«é«”', 'ç»ç’ƒå¿ƒ', 'ç—…äººå¿ƒæƒ…'], 'time': '3 å€‹æœˆå‰'}, {'review': 'é†«ç”Ÿå•è¨ºç”¨å¿ƒï¼Œç­‰å¾…æ™‚é–“ä¹Ÿä¸æœƒå¤ªä¹…ï¼(ğŸ˜Š , + 1)\nå·²ç¶“åœ¨é€™è£¡çœ‹å¥½å¹¾å¹´äº†(ğŸ˜¶ , + 0)\n', 'total': 1, 'key_phrases': ['é†«ç”Ÿ', 'æ™‚é–“'], 'time': '3 å€‹æœˆå‰'}, {'review': 'è”¡å®å½¥é†«å¸«äººå¾ˆå¥½ æœ‰å•å¿…ç­” ï¼(ğŸ˜Š , + 1)\n', 'total': 1, 'key_phrases': ['è”¡å®å½¥é†«å¸«äºº'], 'time': '4 å€‹æœˆå‰'}, {'review': 'é™³é†«å¸«æœ¬äººä¿é¤Šè¶…å¥½çš®è†šç¾éº—åˆ°ç™¼äº®ï¼Œå¾ˆå°ˆæ¥­ä¹Ÿä¸æ¨éŠ·ç”¢å“ï¼Œæ·¨è†šé›·å°„å¾Œå…©å¤©å°±æ²’æœ‰å¾ˆæ˜é¡¯ç—•è·¡ï¼Œçš®è†šä¹Ÿè®Šç™½è®Šäº®ï¼Œè¶…æ€•ç—›çš„æˆ‘éƒ½è¦ºå¾—ä¸‹æ¬¡é‚„æœƒå†å˜—è©¦ï¼(ğŸ˜Š , + 1)\næ¨è–¦ï¼(ğŸ˜Š , + 1)\nğŸ‘(ğŸ˜Š , + 1)\n', 'total': 3, 'key_phrases': ['é™³é†«å¸«æœ¬äºº', 'çš®è†š', 'ç”¢å“', 'æ·¨è†šé›·å°„', 'æ˜é¡¯ç—•è·¡', 'ä¸‹æ¬¡'], 'time': '5 å€‹æœˆå‰'}, {'review': 'æœ‹å‹ä»‹ç´¹æ‰“æ·¨è†šé›·å°„ï¼Œè”¡é†«å¸«æº«æŸ”ç¾éº—å°ˆæ¥­ï¼Œè¡“å¾Œç…§é¡§èªªæ˜æ¸…æ¥šï¼Œæ‰“å®Œæ¢å¾©è‰¯å¥½ï¼Œè†šè‰²æœ‰å‡å‹»ä¸€äº›ï¼Œè‰¯å¥½çš„ç¶“é©—â¤ï¸(ğŸ˜Š , + 1)\n', 'total': 1, 'key_phrases': ['æœ‹å‹', 'æ·¨è†šé›·å°„', 'è”¡é†«å¸«', 'è¡“å¾Œ', 'è†šè‰²', 'è‰¯å¥½', 'ç¶“é©—'], 'time': '5 å€‹æœˆå‰'}, {'review': 'å“‡â‹¯â‹¯è”¡é†«ç”Ÿè¶…æ…ˆç¥¥çš„ï¼Œå¾ˆæœ‰è€å¿ƒçš„å®‰æ’«ç—…æ‚£ï¼(ğŸ˜Š , + 1)\nçœŸæ˜¯é€™é™„è¿‘å¯é‡ä¸å¯æ±‚çš„å¥½é†«ç”ŸğŸ˜­(ğŸ˜  , - 1)\n', 'total': 0, 'key_phrases': ['è”¡é†«ç”Ÿ', 'è€å¿ƒ', 'ç—…æ‚£', 'å¥½é†«ç”Ÿ'], 'time': '8 å€‹æœˆå‰'}, {'review': 'è­·å£«å¾ˆæ„›æ‰¾åŒäº‹èŠå¤©ï¼Œç•¶å¤©æŸè­·å£«æ­£åœ¨å°ˆå¿ƒè¼¸å…¥çœ‹è¨ºè³‡æ–™ï¼Œå¦ä¸€è­·å£«ä¸€ç›´å¹²æ“¾è¦èŠå¤©ï¼Œæ‰“é é˜²é‡ä¹Ÿè‰è‰çµæŸï¼Œçµæœå‡ºä¾†çœ‹åˆ°åˆåœ¨æ»‘æ‰‹æ©Ÿï¼(ğŸ˜Š , + 1)\n', 'total': 1, 'key_phrases': ['è­·å£«', 'åŒäº‹', 'è¨ºè³‡æ–™', 'é é˜²é‡', 'çµæœ', 'æ‰‹æ©Ÿ'], 'time': '9 å€‹æœˆå‰'}]
    # print(finalresult)
  
    unique_time = []
    classify = []
    keywords = {}
    for res in finalresult:
        if res['time'] not in unique_time:
            unique_time.append(res['time'])
            new = {
                'time': res['time'],
                'good': 0,
                'bad': 0,
                'neutral': 0
                }
            if res['total'] > 0:
                new['good']+= 1
            elif res['total'] < 0:
                new['bad'] += 1
            else:
                new['neutral'] += 1
            classify.append(new)
        else:
            temp = [sub for sub in classify if sub['time'] == res['time'] ]
            
            if res['total'] > 0:
                temp[0]['good']+= 1
            elif res['total'] < 0:
                temp[0]['bad'] += 1
            else:
                temp[0]['neutral'] += 1
            
        if res['total'] > 0:
            review['positive'] += 1
        elif res['total'] < 0:
            review['negative'] += 1
        else:
            review['neutral'] += 1

        for i in res['key_phrases']:
            if i not in keywords:
                keywords[i] = 1
            else:
                keywords[i] += 1
        # keywords = {i:res['key_phrases'].count(i) for i in res['key_phrases']}
    print(keywords)


    classify = json.dumps(classify)
    # print(classify)

    # return render_template('reviews_test.html', reviews = result, name = name)

    return render_template('reviews.html', finalresult = finalresult, review = review, name = name, classify = classify)


@handler.add(MessageEvent, message=TextMessageContent)
def message_text(event):
    returnMessages = []
    sentiment_result = azure_sentiment(event.message.text, 'line')
    
    returnMessages.append(TextMessage(
                    text=f"{sentiment_result['review']}ã€ç¸½åˆ†ï¼š{sentiment_result['total']}ã€‘"))
    if sentiment_result['total'] > 0:
        returnMessages.append(TextMessage(text="è¬è¬ä½ çš„è‚¯å®šï¼æˆ‘å€‘ç¹¼çºŒåŠªåŠ› \U00002764"))
    elif sentiment_result['total'] < 0:
        returnMessages.append(TextMessage(text="æŠ±æ­‰è®“ä½ æœ‰ä¸æ„‰å¿«çš„è§€æ„Ÿï¼Œæˆ‘å€‘æ·±å…¥äº†è§£æª¢è¨ \U0001F64F"))
    else:
        returnMessages.append(TextMessage(text="è¬è¬ä½ çµ¦äºˆè©•è«–ï¼æˆ‘å€‘æ”¶åˆ°äº†ï¼ \U0001F609"))
    with ApiClient(configuration) as api_client:
        line_bot_api = MessagingApi(api_client)
        line_bot_api.reply_message_with_http_info(
            ReplyMessageRequest(
                reply_token=event.reply_token,
                messages=returnMessages
                # messages=[TextMessage(text=event.message.text)]
            )
        )

def azure_sentiment(user_input, type):
    text_analytics_client = TextAnalyticsClient(
        endpoint=config["AzureLanguage"]["END_POINT"], credential=credential
    )
    if type == 'line':
        string = user_input
    else:
        string = user_input['review']
    # delimiters = ["ï¼Œ","ã€‚"," ", "\n", ",",".","!","...","â‹¯","ï¼","?","ï¼Ÿ","~","ï½",'|']
    # for delimiter in delimiters:
    #     string = string.replace(delimiter, ' ')
    # documents = string.split()
    #print(documents[:10])
        
    # response = text_analytics_client.analyze_sentiment(
    #     [string], show_opinion_mining=True, language="zh-hant"
    # )
        
    poller = text_analytics_client.begin_analyze_actions(
        [string],
        actions=[
            ExtractKeyPhrasesAction(),
            AnalyzeSentimentAction(),
        ],
        language="zh-Hant",
        polling_interval=1
    )
    document_results = poller.result()
   
    result_list = {}
    
    for doc, action_results in zip([string], document_results):
        sentiment = ''
        point = 0
        result_line = ""
        key_phrases = []
        for result in action_results:
            if result.kind == "KeyPhraseExtraction":
                key_phrases = result.key_phrases
            elif result.kind == "SentimentAnalysis":
                for doc in result.sentences:
                    sentiment = ''
                    if(doc.sentiment == 'positive'):
                        sentiment = '\U0001F60A , + 1'
                        point += 1
                    elif(doc.sentiment == 'negative'):
                        sentiment = '\U0001F620 , - 1'
                        point -= 1
                    else:
                        sentiment = '\U0001F636 , + 0'
                    result_line += (
                        f"{doc.text}({sentiment})\n"
                    )  
                result_list['review'] = result_line
                result_list['total'] = point
                result_list['key_phrases'] = key_phrases


    return result_list

if __name__ == "__main__":
    app.run()

